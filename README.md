# üß† LLM Playground : Comprendre les Grands Mod√®les de Langage

<p align="center">
  <img src="https://img.shields.io/badge/Streamlit-Cloud-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white">
</p>

## üìå Pr√©sentation du Projet
Ce projet est une immersion technique dans le fonctionnement des **Large Language Models (LLM)**. J'ai con√ßu, entra√Æn√© et d√©ploy√© un mod√®le de type **GPT-2** de **39,8 millions de param√®tres**. 

L'objectif est de d√©montrer comment une architecture Transformer apprend des structures linguistiques, depuis la pr√©diction de tokens jusqu'√† l'adoption de registres sp√©cifiques via le **fine-tuning**.

üîó **Acc√©der √† l'application interactive :** HYPERLIEN

---

## üèóÔ∏è Architecture & Sp√©cifications techniques
Le mod√®le, baptis√© **TinyGPT V4**, repose sur les caract√©ristiques suivantes :
* **Structure :** 10 couches (blocks) avec 8 t√™tes d'attention chacune.
* **Dimensions :** $d_{model} = 512$ et une expansion Feed-Forward √† 2048.

---

## üß™ Exp√©rimentations & Fine-Tuning
L'application permet de comparer trois versions du mod√®le pour observer l'impact du dataset sur le style de g√©n√©ration :

| Version | Dataset d'entra√Ænement | Style de sortie |
| :--- | :--- | :--- |
| **Base Model** | TinyStories (500k histoires) | Narratif enfantin  |
| **Wikipedia** | Simple English Wikipedia | Encyclop√©dique / Factuel |
| **TextBook** | Cosmopedia (80%) + TinyStories (20%) | P√©dagogique / √âducatif |

>**Note sur le Catastrophic Forgetting :** Le passage √† la version Wikipedia montre une perte de la capacit√© narrative initiale, un ph√©nom√®ne que j'ai att√©nu√© dans la version TextBook en m√©langeant les datasets.

---
